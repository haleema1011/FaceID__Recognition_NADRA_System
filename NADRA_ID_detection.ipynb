{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Face ID Retrieval System for NADRA\n",
        "**Assignment:** Develop a robust system that retrieves a person's CNIC and information using only their image.  \n",
        "**Dataset:** LFW (from Kaggle)  \n",
        "**Requirements:**\n",
        "- One-shot recognition\n",
        "- Use matching and non-matching pairs for training\n",
        "- Modular inference pipeline with FAISS/KNN\n",
        "- Ability to switch between custom and pretrained backbone\n",
        "- Evaluation of model performance"
      ],
      "metadata": {
        "id": "p5huxIDz4Ick"
      },
      "id": "p5huxIDz4Ick"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages & Imports"
      ],
      "metadata": {
        "id": "Tqy5RXBh4_Tx"
      },
      "id": "Tqy5RXBh4_Tx"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Install Required Packages ------------------------\n",
        "!pip install kagglehub faiss-cpu torch torchvision\n",
        "\n",
        "# ------------------------ Imports ------------------------\n",
        "import os, random, zipfile\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import faiss\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "dmwGCEkD1Qds"
      },
      "id": "dmwGCEkD1Qds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device & Dataset Download"
      ],
      "metadata": {
        "id": "n6WzuCY_5JaW"
      },
      "id": "n6WzuCY_5JaW"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Device ------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "WjGZJE6W1Ss_"
      },
      "id": "WjGZJE6W1Ss_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Download LFW Dataset ------------------------\n",
        "print(\"Downloading LFW dataset from Kaggle...\")\n",
        "path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\n",
        "dataset_dir = path + \"/lfw-deepfunneled/lfw-deepfunneled\"\n",
        "\n",
        "if not os.path.exists(dataset_dir):\n",
        "    zip_path = path + \"/lfw-deepfunneled.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(path)\n",
        "print(\"Dataset ready at:\", dataset_dir)\n"
      ],
      "metadata": {
        "id": "wKrv7jfh1V29"
      },
      "id": "wKrv7jfh1V29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matching & Non-Matching Pairs"
      ],
      "metadata": {
        "id": "KKr0wizU5QGC"
      },
      "id": "KKr0wizU5QGC"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Pair Creation Functions ------------------------\n",
        "def create_matching_pairs_with_filtering(dataset_dir, max_pairs=5000):\n",
        "    matching_pairs, labels = [], []\n",
        "    for person_name in os.listdir(dataset_dir):\n",
        "        person_dir = os.path.join(dataset_dir, person_name)\n",
        "        images = [os.path.join(person_dir, img) for img in os.listdir(person_dir) if img.endswith('.jpg')]\n",
        "        if len(images) > 1:\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i+1, len(images)):\n",
        "                    matching_pairs.append((images[i], images[j]))\n",
        "                    labels.append(1)\n",
        "                if len(matching_pairs) >= max_pairs:\n",
        "                    break\n",
        "        if len(matching_pairs) >= max_pairs:\n",
        "            break\n",
        "    return matching_pairs[:max_pairs], labels[:max_pairs]\n",
        "\n",
        "def create_non_matching_pairs_balanced(dataset_dir, num_matching_pairs, max_pairs=5000):\n",
        "    non_matching_pairs, non_matching_labels = [], []\n",
        "    people = os.listdir(dataset_dir)\n",
        "    num_to_generate = min(num_matching_pairs, len(people)*(len(people)-1)//2, max_pairs)\n",
        "    for _ in range(num_to_generate):\n",
        "        person_1 = random.choice(people)\n",
        "        person_2 = random.choice([p for p in people if p != person_1])\n",
        "        images_1 = [os.path.join(dataset_dir, person_1, img) for img in os.listdir(os.path.join(dataset_dir, person_1)) if img.endswith('.jpg')]\n",
        "        images_2 = [os.path.join(dataset_dir, person_2, img) for img in os.listdir(os.path.join(dataset_dir, person_2)) if img.endswith('.jpg')]\n",
        "        if images_1 and images_2:\n",
        "            img1, img2 = random.choice(images_1), random.choice(images_2)\n",
        "            non_matching_pairs.append((img1, img2))\n",
        "            non_matching_labels.append(0)\n",
        "        if len(non_matching_pairs) >= max_pairs:\n",
        "            break\n",
        "    return non_matching_pairs[:max_pairs], non_matching_labels[:max_pairs]\n",
        "\n",
        "# ------------------------ Generate Pairs ------------------------\n",
        "matching_pairs, matching_labels = create_matching_pairs_with_filtering(dataset_dir, max_pairs=5000)\n",
        "non_matching_pairs, non_matching_labels = create_non_matching_pairs_balanced(dataset_dir, len(matching_pairs), max_pairs=5000)\n",
        "all_pairs = matching_pairs + non_matching_pairs\n",
        "all_labels = matching_labels + non_matching_labels\n",
        "print(f\"Matching pairs: {len(matching_pairs)}, Non-matching pairs: {len(non_matching_pairs)}\")\n",
        "print(f\"Total pairs: {len(all_pairs)}\")"
      ],
      "metadata": {
        "id": "9SgGm6cw1cUG"
      },
      "id": "9SgGm6cw1cUG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class & DataLoader"
      ],
      "metadata": {
        "id": "pXOUCRvI5Vwh"
      },
      "id": "pXOUCRvI5Vwh"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Dataset Class ------------------------\n",
        "class LFWPairsDataset(Dataset):\n",
        "    def __init__(self, pairs, labels, transform=None):\n",
        "        self.pairs = pairs\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path = self.pairs[idx]\n",
        "        label = self.labels[idx]\n",
        "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
        "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "d-9eWbya1gBR"
      },
      "id": "d-9eWbya1gBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Siamese Network & Training"
      ],
      "metadata": {
        "id": "LfKYJff95b8J"
      },
      "id": "LfKYJff95b8J"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Transforms and DataLoader ------------------------\n",
        "transform = transforms.Compose([transforms.Resize((100,100)), transforms.ToTensor()])\n",
        "dataset = LFWPairsDataset(all_pairs, all_labels, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "QPK4YNvu1ite"
      },
      "id": "QPK4YNvu1ite",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Pretrained Backbone ------------------------\n",
        "class Backbone(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True):\n",
        "        super(Backbone, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.fc = nn.Linear(resnet.fc.in_features, embedding_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zx8yZRYh1mIW"
      },
      "id": "zx8yZRYh1mIW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Contrastive Loss ------------------------\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
        "        loss = label * torch.pow(euclidean_distance, 2) + (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0),2)\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "HsjvUO8g1pt4"
      },
      "id": "HsjvUO8g1pt4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Siamese Network Training ------------------------\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.backbone = Backbone(embedding_dim=embedding_dim, pretrained=pretrained)\n",
        "    def forward_once(self, x):\n",
        "        return self.backbone(x)\n",
        "    def forward(self, x1, x2):\n",
        "        return self.forward_once(x1), self.forward_once(x2)\n",
        "\n",
        "model = SiameseNetwork().to(device)\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(5):  # fewer epochs to see output quickly\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (img1, img2, labels) in enumerate(dataloader):\n",
        "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out1, out2 = model(img1, img2)\n",
        "        loss = criterion(out1, out2, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}\")\n",
        "    print(f\"Epoch {epoch+1} completed, Average Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kza2ixt4sgAg",
        "outputId": "90394246-96ed-4300-cc35-3b47722e77ff"
      },
      "id": "kza2ixt4sgAg",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.8.3)\n",
            "Using device: cuda\n",
            "Downloading LFW dataset from Kaggle...\n",
            "Using Colab cache for faster access to the 'lfw-dataset' dataset.\n",
            "Dataset ready at: /kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching pairs: 5000, Non-matching pairs: 5000\n",
            "Total pairs: 10000\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 234MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1, Batch 0, Loss: 22.4602\n",
            "Epoch 1, Batch 50, Loss: 0.2165\n",
            "Epoch 1, Batch 100, Loss: 1.1209\n",
            "Epoch 1, Batch 150, Loss: 0.1901\n",
            "Epoch 1, Batch 200, Loss: 0.1562\n",
            "Epoch 1, Batch 250, Loss: 0.1562\n",
            "Epoch 1, Batch 300, Loss: 0.1090\n",
            "Epoch 1 completed, Average Loss: 1.8603\n",
            "Epoch 2, Batch 0, Loss: 0.1030\n",
            "Epoch 2, Batch 50, Loss: 0.0826\n",
            "Epoch 2, Batch 100, Loss: 0.0885\n",
            "Epoch 2, Batch 150, Loss: 0.0262\n",
            "Epoch 2, Batch 200, Loss: 0.0947\n",
            "Epoch 2, Batch 250, Loss: 0.0211\n",
            "Epoch 2, Batch 300, Loss: 0.0243\n",
            "Epoch 2 completed, Average Loss: 0.0760\n",
            "Epoch 3, Batch 0, Loss: 0.1170\n",
            "Epoch 3, Batch 50, Loss: 0.0697\n",
            "Epoch 3, Batch 100, Loss: 0.0081\n",
            "Epoch 3, Batch 150, Loss: 0.0078\n",
            "Epoch 3, Batch 200, Loss: 0.0050\n",
            "Epoch 3, Batch 250, Loss: 0.0266\n",
            "Epoch 3, Batch 300, Loss: 0.0599\n",
            "Epoch 3 completed, Average Loss: 0.0418\n",
            "Epoch 4, Batch 0, Loss: 0.0459\n",
            "Epoch 4, Batch 50, Loss: 0.0236\n",
            "Epoch 4, Batch 100, Loss: 0.0493\n",
            "Epoch 4, Batch 150, Loss: 0.0176\n",
            "Epoch 4, Batch 200, Loss: 0.0221\n",
            "Epoch 4, Batch 250, Loss: 0.0142\n",
            "Epoch 4, Batch 300, Loss: 0.0085\n",
            "Epoch 4 completed, Average Loss: 0.0254\n",
            "Epoch 5, Batch 0, Loss: 0.0184\n",
            "Epoch 5, Batch 50, Loss: 0.0049\n",
            "Epoch 5, Batch 100, Loss: 0.0239\n",
            "Epoch 5, Batch 150, Loss: 0.0773\n",
            "Epoch 5, Batch 200, Loss: 0.0044\n",
            "Epoch 5, Batch 250, Loss: 0.0236\n",
            "Epoch 5, Batch 300, Loss: 0.0327\n",
            "Epoch 5 completed, Average Loss: 0.0203\n",
            "Code is ready. You can now build embeddings and test single-shot recognition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Embeddings Database (FAISS/KNN)"
      ],
      "metadata": {
        "id": "s977AJmp4oGj"
      },
      "id": "s977AJmp4oGj"
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------Embeddings--------------------------------\n",
        "def build_embedding_db(model, image_dict, transform, device):\n",
        "    model.eval()\n",
        "    embeddings, ids = [], []\n",
        "    for person, imgs in image_dict.items():\n",
        "        for img_path in imgs:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img = transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                emb = model.forward_once(img).cpu().numpy()  # <-- fix here\n",
        "            embeddings.append(emb)\n",
        "            ids.append(person)\n",
        "    embeddings = np.vstack(embeddings).astype('float32')\n",
        "    faiss_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    faiss_index.add(embeddings)\n",
        "    knn_index = embeddings\n",
        "    return embeddings, ids, faiss_index, knn_index\n"
      ],
      "metadata": {
        "id": "Uw0d9M0CwCmO"
      },
      "id": "Uw0d9M0CwCmO",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recognize_face(model, img_path, transform, device, ids, knn_index=None, faiss_index=None, method='faiss', k=1):\n",
        "    model.eval()\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = model.forward_once(img).cpu().numpy().astype('float32')  # <-- use forward_once\n",
        "    if method=='faiss':\n",
        "        D,I = faiss_index.search(emb,k)\n",
        "        return [ids[i] for i in I[0]], D[0]\n",
        "    elif method=='knn':\n",
        "        distances = np.linalg.norm(knn_index - emb, axis=1)\n",
        "        idx = np.argsort(distances)[:k]\n",
        "        return [ids[i] for i in idx], distances[idx]"
      ],
      "metadata": {
        "id": "8m1pgOOgvBxs"
      },
      "id": "8m1pgOOgvBxs",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ Build Embedding Database ------------------------\n",
        "# use only 1 image per person (single-shot)\n",
        "image_dict = {}\n",
        "for person in os.listdir(dataset_dir):\n",
        "    person_dir = os.path.join(dataset_dir, person)\n",
        "    images = [os.path.join(person_dir, img) for img in os.listdir(person_dir) if img.endswith('.jpg')]\n",
        "    if images:\n",
        "        image_dict[person] = [images[0]]  # take the first image only for embedding\n",
        "\n",
        "print(\"Building embedding database...\")\n",
        "embeddings, ids, faiss_index, knn_index = build_embedding_db(model, image_dict, transform, device)\n",
        "print(\"Embedding database ready.\")\n",
        "\n",
        "# ------------------------ Pick a random test image ------------------------\n",
        "import random\n",
        "test_person = random.choice(list(image_dict.keys()))\n",
        "test_img_path = random.choice(image_dict[test_person])\n",
        "print(f\"Testing image from person: {test_person}\")\n",
        "Image.open(test_img_path).show()\n",
        "\n",
        "# ------------------------ Predict using FAISS ------------------------\n",
        "pred_ids_faiss, distances_faiss = recognize_face(model, test_img_path, transform, device, ids, faiss_index=faiss_index, method='faiss', k=1)\n",
        "print(\"FAISS Prediction:\", pred_ids_faiss, \"Distance:\", distances_faiss)\n",
        "\n",
        "# ------------------------ Predict using KNN ------------------------\n",
        "pred_ids_knn, distances_knn = recognize_face(model, test_img_path, transform, device, ids, knn_index=knn_index, method='knn', k=1)\n",
        "print(\"KNN Prediction:\", pred_ids_knn, \"Distance:\", distances_knn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtoGZo1UubO0",
        "outputId": "2e232ec4-7d11-4d35-da87-15564994a7df"
      },
      "id": "HtoGZo1UubO0",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building embedding database...\n",
            "Embedding database ready.\n",
            "Testing image from person: Eliane_Karp\n",
            "FAISS Prediction: ['Eliane_Karp'] Distance: [0.]\n",
            "KNN Prediction: ['Eliane_Karp'] Distance: [0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "afoKY_vR54Gb"
      },
      "id": "afoKY_vR54Gb"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "def evaluate_siamese(model, dataloader, device, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate Siamese Network on a dataloader of pairs\n",
        "    Returns accuracy, ROC-AUC, and prints matching/non-matching performance\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, labels in dataloader:\n",
        "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
        "            out1, out2 = model(img1, img2)\n",
        "            # Compute Euclidean distance\n",
        "            dist = nn.functional.pairwise_distance(out1, out2)\n",
        "            # Predict match if distance < threshold\n",
        "            preds = (dist < threshold).float()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, -np.array(all_preds))  # negative for distance\n",
        "    except:\n",
        "        auc = None\n",
        "    print(f\"Evaluation -> Accuracy: {acc*100:.2f}%, ROC-AUC: {auc}\")\n",
        "    return acc, auc\n"
      ],
      "metadata": {
        "id": "tXWGNrVr1xWM"
      },
      "id": "tXWGNrVr1xWM",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#using 10% of dataset for evaluation\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "val_size = int(0.1 * len(dataset))\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Evaluate\n",
        "evaluate_siamese(model, val_loader, device, threshold=1.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ySd4oA2YkY",
        "outputId": "0bf25932-a197-4269-e15e-ed1cb44e2ea1"
      },
      "id": "c0ySd4oA2YkY",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation -> Accuracy: 93.90%, ROC-AUC: 0.057874762808349134\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.939, np.float64(0.057874762808349134))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single-Shot Retrieval Evaluation"
      ],
      "metadata": {
        "id": "oDQupNcD3ZAd"
      },
      "id": "oDQupNcD3ZAd"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval(model, image_dict, transform, device, method='faiss', k=1):\n",
        "    \"\"\"\n",
        "    Evaluate single-shot recognition using FAISS or KNN\n",
        "    Returns top-1 accuracy\n",
        "    \"\"\"\n",
        "    # Build embeddings database\n",
        "    embeddings, ids, faiss_index, knn_index = build_embedding_db(model, image_dict, transform, device)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for person, imgs in image_dict.items():\n",
        "        for img_path in imgs:\n",
        "            total += 1\n",
        "            pred_ids, distances = recognize_face(model, img_path, transform, device, ids, knn_index=knn_index, faiss_index=faiss_index, method=method, k=k)\n",
        "            if person in pred_ids:  # top-1 match\n",
        "                correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"{method.upper()} single-shot retrieval accuracy: {accuracy*100:.2f}%\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "s3Nb41bk240h"
      },
      "id": "s3Nb41bk240h",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For demo, use a subset of image_dict if too large\n",
        "subset_dict = {k: image_dict[k] for i, k in enumerate(list(image_dict.keys())[:50])}\n",
        "\n",
        "# Evaluate FAISS\n",
        "evaluate_retrieval(model, subset_dict, transform, device, method='faiss', k=1)\n",
        "\n",
        "# Evaluate KNN\n",
        "evaluate_retrieval(model, subset_dict, transform, device, method='knn', k=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jjw0bOw29lZ",
        "outputId": "41033ecc-62f5-481c-94c7-b56b6f9aa719"
      },
      "id": "5jjw0bOw29lZ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS single-shot retrieval accuracy: 100.00%\n",
            "KNN single-shot retrieval accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}